default:
  networksize: [128, 256]
  states: ["distance1", "distance2"]

medium-size:
  networksize: [128, 256]
  states: ["distance1", "distance2"]

distance-to-vector:
  networksize: [256, 512]
  changed_values:
    states: ["vector1", "vector2"]

add-joints:
  networksize: [256, 512]
  changed_values:
    states: ["ditance1", "distance2", "five of joints"]

joints-vector:
  networksize: [256, 512].                    
  changed_values:
    states: ["vector1", "vector2", "five of joints"]

decrease_networksize:
  networksize: [64, 128].                    
  changed_values:
    states: ["vector1", "vector2", "five of joints"]

baseline:
  networksize: [256, 512].                    
  changed_values:
    states: ["vector1", "vector2", "five of joints"]
    reward: "+1 if distance1 < 0.085"
    joint_increment_value: 0.2

change_state:
  *: "same as above"
  states: ["distance1", "distance2"]

add_joints:
  *: "same as above"
  states: ["distance1", "distance2", "five of joints"]

non_additional_reward:
  *: "same as above"
  state: ["distance1", "distance2"]
  reward: null

2022-11-medium_network_size:
  *: "same as above"
  networksize: [128, 256]
  states: ["vector1", "vector2", "five of joints"]
  reward: "+1 if distance1 <= 0.085"

baseline200:
  *: "same as above"
  networksize: [256, 512]
  nepisodes: 200

positive_reward:
  *: "same as above"
  networksize: [256, 512]
  reward: "positive reward"

epsilon_greedy_off:
  *: "same as above"
  reward: "+1 if distance1 <= 0.085"
  epsilon: off

positive_epsilon_off:
  *: "same as above"
  reward: "positive reward"
  epsilon: off
  nepisodes: 200

can_negative_rewaard:
  reward: "negative reward"
  epsilon: off

first_with_human:
  shared: 
    reward: "positive reward"
    epsilon: off
    nvideos: 20
    additional_reward: "defference between human and robot joint"  
    loss: null

  trimmed_human:
    videos: "trimmed videos"

  loss_joint:
    additional_reward: null
    loss: "difference between human and robot joint"
  
  loss_joint_average_rewaard:
    additional_reward: null
    loss: "difference between human and robot joint"
    average_reward: "To compare, added figure of average_reward"
  
  reward_video_loop:
    additional_reward: "difference between human and robot joint"
    video_loop: "1 episode runs 20 videos"
  
human_only:
  shared:
    nvideos: 20
    reward: "positive reward"
  first_human_only:
    description: "after the human_only learning, run 500 episodes of learning in Pepper env"
    
positive_epsilon_off_500:
  nepisodes: 500
  reward: before - 0.005 <= distance <= before + 0.005

positive_epsilon_off_500_round:
  nepisodes: 500
  reward: round(before, 10) == round(distance, 10)

human_correrct_initial_joint:
  description: "I modified initial joint RightElbowYaw angle 
    because it was too big to move other joint angle"

big_rshoulderroll_limit:
  description: "I make larger rshoulder roll limits"

direct_to_cube_with_human:
  description: "mistook learning. I forgot changing name of model"

direct_to_cube_with_human2:
  description: "correct learning for above one"

direct_to_cube_with_human2:
  description: "Just use 500 episodes of results of above one"

human_only_loop:
  description: "something is wrong"
  nepisodes: 100

with_human_only_loop:
  description: "something is wrong"
  description: "add for loop to learn many times for 1 video"

human_only_loop_50:
  description: "something is wrong"
  nepisodes: 50

dropout_0.8:
  description: "add drop out method for Actor and Critic model
    . Only pepper environment"
  dropped_out_value(p): 0.8

direct_to_cube_30_steps:
  description: "no dropout. direct_to_cube"

direct_to_cube_50_steps:
  description: "same as above. nstep is 50"

random_cube:
  step: 40
  description: "the first time on random cube"

baselinee_40steps:
  step: 40
  description: "the baseline of 40 steps not random cube.
    I changed how to move joints not to use joint pose."

baseline_40steps2:
  step: 40
  description: "same as above one"

random_cube2:
  step: 40
  description: "back to move joints to use current joint pose.
    random cube"

# From here, important learning

distance:
  description: "states are only distances"

difficult_random_cube:
  description: "change direction of cube to target
    states are vector and joint"

add_joints:
  description: "states are distances and joint"

vectors:
  description: "states are vectors"

joints_and_vectors:
  description: "states are joints and vectors"

joints_and_vectors_500:
  description: "states are joints and vectors. 
    The difference between above and this is only the number of episodes"

epsilon_greedy:
  description: "add epsilon-greedy method. epsilon begin is 0.2, and epsilon end is 0.01 "

epsilon_greedy_with_human:
  description: "add epsilon-greedy method. the same to above. Difference is with human"

with_human:
  description: "with human. Non epsilon-greedy"

little_random_cube:
  description: "little random cube."

little_random_with_human:
  desccription: "little random cube with human."

little_random_with_human_500:

little_random_cube_500:

with_human_500:

random_cube:
  description: "random cube"